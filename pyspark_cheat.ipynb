{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheatsheet for `pyspark`\n",
    "A super-synthetic cheatsheet with the most important commands for `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.select('col1', 'col4', 'col7')` - to select only certain columns <br>\n",
    "`.select(df.col1.cast('integer), df.col2.cast('double))` - select only certain columns and change their type <br>\n",
    "`.filter(boolean)` - filter our rows that for which the `boolean` statement is `False` <br>\n",
    "`.distinct()` -  to select only distinct rows <br>\n",
    "`.sort('col')` - to sort data in ascending order of `'col` <br>\n",
    "`.groupby(['col1', 'col2'])` - to group data by columns (usually followed by another action like `.sum()`, `.count()`, etc.)<br>\n",
    "`.show(5)` - shows the first 5 lines of the dataset <br>\n",
    "`.printSchema()` - shows the columns and their type <br>\n",
    "`.coalesce(1)` - puts all data on 1 partition only <br>\n",
    "`.withColumn(columName, function)` - creates a new column named `columnName` as a result of the operation specified by `function` <br>\n",
    "`.persist()` - ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyspark.ml.feature`\n",
    "A series of methods for transforming datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label enconding** converts a categorical attribute to a number.\n",
    "```\n",
    "indexer = StringIndexer(inputCol=..., outputCol=...)\n",
    "indexer = indexer.fit(df)\n",
    "df_idx = indexer.transform(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**\n",
    "```from pysparl.ml.feature import OneHotEncoderEstimator\n",
    "ohe = OneHotEncoderEstimator(inputCols=[...], outputCols=[...])\n",
    "ohe = ohe.fit(df)\n",
    "df_ohe = ohe.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucketing**\n",
    "```\n",
    "bkt = Bucketizer(splits=[val1, val2, val3,...], inputCol='col', outputCol='col_bkt')\n",
    "df = bkt.transform(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing & remove stop words**\n",
    "```\n",
    "tokenizer = Tokenizer(inputCol=..., outputCol=...)\n",
    "tokenizer = tokenizer.fit(df)\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=..., outputCol=...)\n",
    "remover = remover.fit(df)\n",
    "df = remover.transform(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**\n",
    "```\n",
    "hasher = HashingTF(inputCol=..., outputCol=...)\n",
    "hasher = hasher.fit(df)\n",
    "df = hasher.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=..., outputCol=...)\n",
    "idf = idf.fit(df)\n",
    "df = idf.transform(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector assembler** allows to assemble all the feature columns into a single `feature` column which is the required format by `pyspark`'s ML algorithm.\n",
    "\n",
    "```\n",
    "assemble = VectorAssembler(inputCols=[...], outputCol='features')\n",
    "X_train = assemble.transform(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyspark.ml.regression`\n",
    "A series of ML techniques for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "reg = LinearRegression(labelCol='col').fit(X_train)\n",
    "pred = reg.transform(X_test)\n",
    "RegressionEvaluator(metricName='rmse', labelCol='col', predictionCol='predictions').evaluate(pred)\n",
    "```\n",
    "\n",
    "- As a consequence of `.fit()`, `reg` contains an `.intercept` and an `.coefficients` attribute. Be carefule with the order of `.coefficients` elements, especially with one-hot encoded features.\n",
    "- As a consequence of `.transform()`, `pred` contains a column named `prediction` which is the output of the model.\n",
    "- `RegressionEvaluator` computes the RMSE by default, but could also compute `mae`, `r2`, `mse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyspark.ml.classification`\n",
    "\n",
    "```\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "form pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "gbt = GBTClassifier().fit(X_train)\n",
    "preds = gbt.transform(X_test)\n",
    "BinaryClassifierEvaluator().evaluate(preds)\n",
    "```\n",
    "- `BinaryClassifierEvaluator()` evaluates the `AUC` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyspark.ml.tuning`\n",
    "A series of routines for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "reg = LinearRegression(labelCol='col')\n",
    "evaluator = RegressionEvaluator(labelCol='col')\n",
    "\n",
    "params = ParamGridBuilder()\n",
    "params = params.addGrid(reg.fitIntercept, [True, False])\n",
    "params = params.build()\n",
    "\n",
    "cv = CrossValidator(estimator=reg, \n",
    "                    estimatorParamMaps=params, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5, \n",
    "                    seed=42)\n",
    "cv.fit(X_train)\n",
    "print(cv.bestModel)\n",
    "print(cv.bestModel.stages) #if estimator is a pipeline\n",
    "print(cv.bestModel.explainParam('fitIntercept))\n",
    "print(cv.avgMetrics)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88e4eedf6ae28bec8f1b745e43fd10f104294c44e6ed6f7a79a4599096f91349"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cheatsheets': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
